# ğŸ˜ƒ Agent ç³»ç»Ÿä¼˜åŒ–è°ƒç ”

### Cataglories

* multi-agent workflow design
  * Magentic-one: A generalist multi-agent system for solving complex tasks. 2024.
  * Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents. 2025.
  * PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods. 2024.
  * Very Large-Scale Multi-Agent Simulation in AgentScope. arXiv:2407
* RAG optimization (specific)
  * llm prefill latency
    * RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation. arXiv:2404
      * rag prefix kv-cache
* Compound AI optimization
  * framework design
    * Towards Resource-Efficient Compound AI Systems. HOTOS 25
      * AIWaas design, Workflow-Aware Cluster Management.
  * SLO Goodput
    *
  * e2e latency
    * Towards Efficient Compound Large Language Model System Serving in the Wild. IWQoS'2024
      * DAG is uncertain, including dependency and exec time; priority schedule most uncertain request to parallel consequential stage;
    * LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM Applications. arXiv:2504
      * uncertainty aware schdule to minimize average JCT
    * Efficient Serving of LLM Applications with Probabilistic Demand Modeling. Arxiv:2506
      * uncertainty modeling to better schdule and prewarm backend to minimize JCT
  * llm prefill latency
    * KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows. Arxiv:2507
      * agent prefix kv-cache cache

### papers

> Tempo: Application-aware LLM Serving with Mixed SLO Requirements

ç°æœ‰3ç§è¯·æ±‚ç±»å‹

* **å»¶è¿Ÿæ•æ„Ÿå‹è¯·æ±‚** ï¼šå¦‚äº¤äº’å¼èŠå¤©æœºå™¨äººï¼ˆChatGPTï¼‰ã€å®æ—¶è¯­éŸ³è½¬æ–‡å­—æœåŠ¡ï¼Œå…³æ³¨ **é€ token ç”Ÿæˆå»¶è¿Ÿ** ï¼ˆTTFTã€TBTï¼‰ï¼Œéœ€é€æ­¥æµå¼å“åº”ã€‚
* **ååå¯†é›†å‹è¯·æ±‚** ï¼šå¦‚å¤§è§„æ¨¡ä»£ç ç”Ÿæˆã€æ‰¹é‡æ•°æ®å¤„ç† APIï¼Œéœ€åœ¨æŒ‡å®šæ—¶é—´å†…å®Œæˆæ•´ä¸ªå“åº”ï¼ˆTTLTï¼‰ã€‚
* **é›†ä½“è¯·æ±‚** ï¼šå¦‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæˆ–å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆTree of Thoughtsï¼‰ï¼Œæ¶‰åŠå¤šä¸ª LLM è°ƒç”¨çš„åŠ¨æ€ä¾èµ–å…³ç³»DAGï¼Œéœ€æ•´ä½“ç«¯åˆ°ç«¯å»¶è¿Ÿæ»¡è¶³ SLOã€‚

ç°æœ‰è°ƒåº¦å™¨ï¼ˆå¦‚ Sarathi-Serveã€Autellixï¼‰é€šå¸¸é’ˆå¯¹å•ä¸€ç±»å‹è¯·æ±‚ä¼˜åŒ–ï¼š

* **å»¶è¿Ÿæ„ŸçŸ¥è°ƒåº¦å™¨** ï¼šä¼˜å…ˆå¤„ç†ä½ TTFT/TBT è¯·æ±‚ï¼Œä½†å¯èƒ½ç‰ºç‰²ååå¯†é›†å‹è¯·æ±‚çš„æ€§èƒ½ã€‚
* **ååæ„ŸçŸ¥è°ƒåº¦å™¨** ï¼šä¼˜åŒ– TTLTï¼Œä½†å¯èƒ½å¯¼è‡´å»¶è¿Ÿæ•æ„Ÿè¯·æ±‚çš„ SLO è¿è§„ã€‚
* **èµ„æºæµªè´¹** ï¼šä¸ºç¡®ä¿ SLO ä¸¥æ ¼æ»¡è¶³ï¼Œå¯èƒ½è¿‡åº¦åˆ†é…èµ„æºï¼Œé™ä½å…¶ä»–è¯·æ±‚çš„æ•ˆç‡ã€‚

motivationï¼š**LLMè¯·æ±‚çš„å¤šæ ·æ€§ä¸SLOéœ€æ±‚çš„å¤æ‚æ€§**

* **è·¨åº”ç”¨å·®å¼‚** ï¼šä¸åŒåº”ç”¨å¯¹SLOçš„éœ€æ±‚æ˜¾è‘—ä¸åŒï¼ˆå¦‚èŠå¤©æœºå™¨äººä¾§é‡å»¶è¿Ÿï¼Œæ‰¹é‡å¤„ç†ä¾§é‡ååï¼‰ã€‚
* **åŒä¸€åº”ç”¨å†…å·®å¼‚** ï¼š
  * èŠå¤©æœºå™¨äººç”¨æˆ·å› é˜…è¯»é€Ÿåº¦ä¸åŒï¼ŒTBTéœ€æ±‚å·®å¼‚æ˜¾è‘—ï¼ˆ38.1%ç”¨æˆ·åå¥½æµå¼ä»£ç äº¤ä»˜ï¼Œ30.5%ç”¨æˆ·éœ€å¿«é€Ÿå…¨å“åº”ï¼‰ã€‚
  * å®šä»·å½±å“ç›¸åº”æ—¶é—´
  * å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œåˆå§‹â€œæ€è€ƒâ€é˜¶æ®µéœ€å¿«é€Ÿå“åº”ï¼ˆå¦‚5ç§’å†…å®Œæˆå†…éƒ¨æ¨ç†ï¼‰ï¼Œè€Œåç»­æ€»ç»“é˜¶æ®µéœ€æµç•…ç”Ÿæˆã€‚

æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡é‡åŒ–æœåŠ¡å¢ç›Šï¼ˆService Gainï¼‰ï¼ŒåŠ¨æ€åˆ†é…â€œæ°åˆ°å¥½å¤„â€çš„èµ„æºæ»¡è¶³SLOï¼Œæœ€å¤§åŒ–å‰©ä½™èµ„æºæœåŠ¡å…¶ä»–è¯·æ±‚ã€‚

æŒ‘æˆ˜ï¼š

* **è¯·æ±‚ä¸ç¡®å®šæ€§çš„æŒ‘æˆ˜ï¼šåŒ…æ‹¬DAGï¼Œé•¿åº¦é¢„æµ‹ã€è¯·æ±‚è´Ÿè½½å˜åŒ–**

> Optimizing SLO-oriented LLM Serving with PD-Multiplexing

åœºæ™¯ï¼šå¤šè½®å¯¹è¯kv-cacheé‡ç”¨ï¼Œé•¿åºåˆ—

è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ LLM æœåŠ¡ä¸­ **SLO ä¿è¯**ä¸**é«˜ååé‡**çš„å†²çª

PD sloçš„åŒºåˆ«å¯¼è‡´å¤©ç„¶éœ€è¦PDåˆ†ç¦»ï¼Œè®¡ç®—ç‰¹æ€§åŸå› å¯¼è‡´SLOä¸‹çš„å¹¶è¡Œç­–ç•¥ã€PDå®ä¾‹æ¯”ä¾‹çš„ä¸åŒç­‰

é—®é¢˜ï¼škv cacheé‡ç”¨ä¸‹ï¼Œéœ€è¦å°†Dâ†’Pï¼Œæ— æ³•layer-wiseä¼ è¾“ï¼ˆPâ†’Dï¼‰ï¼Œå¯¼è‡´å¤šè½®å¯¹è¯è¯·æ±‚é«˜latencyã€‚å¦åˆ™å°±éœ€è¦é‡è®¡ç®—kv-cache

å¦ä¸€ä¸ªæ–¹æ³•æ˜¯chunk-prefillï¼Œä½†æ˜¯åœ¨SLOä¸‹æ— æ³•æ»¡è¶³GPUçš„é«˜åˆ©ç”¨ç‡â†’ååé‡ä¸å¤Ÿ,åŒæ—¶attentionè®¡ç®—å¤æ‚åº¦å˜åŒ–

æœ¬æ–‡æ ¸å¿ƒæ€æƒ³ï¼šspatial prefill-decode (PD) multiplexing approach.ï¼Œåˆ©ç”¨nvidiaå®˜æ–¹`GreenContext`åˆ†å‰²æ–¹æ³•ï¼Œåˆ†å‰²è®¡ç®—æ ¸ï¼Œä½†ä¸åˆ†å‰²å­˜å‚¨ã€‚æ—¢å¯ä»¥PDåˆ†ç¦»ï¼Œåˆå¯ä»¥æ— kv-cacheè¿ç§»å¼€é”€

æŒ‘æˆ˜ï¼šå¦‚ä½•è°ƒåº¦è¯·æ±‚æ»¡è¶³SLOå¹¶MAXååé‡ï¼Œå³å»ºæ¨¡ï¼›å¦‚ä½•åˆ’åˆ†ï¼›å¦‚ä½•åŒæ­¥ï¼Œå³è¯·æ±‚ä»pâ†’d

æ¶æ„ï¼š

* **ç¦»çº¿å»ºæ¨¡**ï¼šé€šè¿‡ GreenContextï¼ˆNVIDIA æä¾›çš„ GPU å†…éƒ¨ç©ºé—´åˆ’åˆ†æŠ€æœ¯ï¼‰é¢„è®­ç»ƒ latencyé¢„æµ‹æ¨¡å‹ï¼Œé‡åŒ–ä¸åŒè®¡ç®—åˆ†ç‰‡æ¯”ä¾‹ä¸‹ prefill/decode çš„æ‰§è¡Œæ—¶é—´ã€‚å‚è€ƒloongserveå»ºæ¨¡
* **åœ¨çº¿æœåŠ¡**ï¼š
  * **SLO æ„ŸçŸ¥è°ƒåº¦å™¨**ï¼šæ ¹æ®é¢„æµ‹å™¨çš„é¢„æµ‹ä¿¡æ¯ï¼ŒåŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼ˆåˆ’åˆ†æ–¹æ¡ˆï¼‰ï¼Œä¼˜å…ˆä¿è¯ decode é˜¶æ®µçš„ SLOã€‚
  * **è‡ªé€‚åº”ç»„è°ƒåº¦**ï¼šå°† prefill é˜¶æ®µåˆ‡åˆ†ä¸ºå¤šä¸ªå—ï¼ˆblocksï¼‰ï¼Œä¸ decode é˜¶æ®µå¹¶è¡Œæ‰§è¡Œï¼Œå‡å°‘èµ„æºæµªè´¹ã€‚

#### **è‡ªé€‚åº”Gangè°ƒåº¦ï¼ˆAdaptive Gang Schedulingï¼‰**

* **åˆ†å—é¢„å¡«å……**ï¼šå°†é•¿é¢„å¡«å……é˜¶æ®µæ‹†åˆ†ä¸ºå¤šä¸ªå—ï¼ˆPBsï¼‰ï¼Œæ¯ä¸ªå—ç‹¬ç«‹è°ƒåº¦ã€‚å‡å°decodeç­‰å¾…prefillå®Œæˆçš„æ°”æ³¡ï¼ŒåŒæ—¶å‡å°kernel launchå¯¼è‡´çš„decodeé—´çš„æ°”æ³¡
* **åŠ¨æ€åŒæ­¥**ï¼šé€šè¿‡æŸ¥è¯¢CUDAäº‹ä»¶çŠ¶æ€å®ç°éé˜»å¡åŒæ­¥ï¼Œé¿å…å› é¢„å¡«å……å’Œè§£ç é˜¶æ®µå®Œæˆæ—¶é—´å·®å¼‚å¯¼è‡´çš„â€œæ°”æ³¡â€ï¼ˆèµ„æºç©ºé—²ï¼‰ã€‚
* **ä¼˜å…ˆçº§è°ƒåº¦**ï¼šçŸ­è¯·æ±‚å¯æŠ¢å é•¿é¢„å¡«å……å—çš„æ‰§è¡Œï¼Œé¿å…SLOè¿åã€‚

![image.png](attachment:3ffa1521-a821-4ee1-8ae5-a5b090a8290e:image.png)

è¿™éƒ¨åˆ†ä¸ºä»€ä¹ˆä¸èƒ½å¹¶è¡Œçš„å¯åŠ¨kernelæœ‰ç‚¹ç–‘é—®ã€‚å¯èƒ½æ˜¯è¿™ä¸ªåŸå› ï¼šGreenContext æ˜¯ NVIDIA æä¾›çš„ä¸€ç§**è¿›ç¨‹å†… GPU è®¡ç®—èµ„æºåˆ†åŒºæŠ€æœ¯**ï¼Œå…è®¸åœ¨åŒä¸€ä¸ª CUDA è¿›ç¨‹ä¸­åˆ›å»ºå¤šä¸ªè™šæ‹Ÿçš„â€œä¸Šä¸‹æ–‡â€ï¼ˆGreen Contextsï¼‰ï¼Œæ¯ä¸ªä¸Šä¸‹æ–‡å¯ä»¥ç‹¬ç«‹åˆ†é… SMsï¼ˆæµå¤„ç†å™¨ï¼‰èµ„æºã€‚å…¶æ ¸å¿ƒè®¾è®¡ç›®æ ‡æ˜¯**åœ¨åŒä¸€è¿›ç¨‹ä¸­å®ç°å¤šä¸ªä»»åŠ¡çš„ç©ºé—´å¤ç”¨**ï¼Œè€Œéè·¨è¿›ç¨‹éš”ç¦»ã€‚

> Chameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models

**ç¬¬ä¸€ç±» RALMï¼šæ£€ç´¢â€œæ–‡æœ¬å—â€ï¼Œå•è¯æ£€ç´¢ï¼Œå¤šæ¬¡é—´éš”æ£€ç´¢**

**ç¬¬äºŒç±» RALMï¼šä»…æ£€ç´¢â€œä¸‹ä¸€ä¸ª tokenâ€ï¼Œæ£€ç´¢æ•°æ®åº“ç±»ä¼¼ä¸Šä¸‹æ–‡çš„ä¸‹ä¸€ä¸ªtokenï¼Œqueryæ˜¯æœ€åä¸€å±‚çš„hidden stateï¼Œlogitsä¸ä¸‹ä¸€ä¸ªtokenåšå¹³å‡**

å‘é‡æœç´¢çš„å®šä¹‰

A vector search takes a ğ·-dimensional query vector ğ‘¥ as input and retrieves ğ¾ similar vector(s) from a database ğ‘Œ, populated with many ğ·-dimensional vectors, based on metrics like L2 distances or cosine similarity.

ğ¾ nearest neighborå¤ªæ˜‚è´µï¼Œä¸šç•Œå¾€å¾€ç”¨approximate nearest neighbor (ANN)ã€‚ANNæ•ˆæœä¸€èˆ¬ç”¨recall at ğ¾ (ğ‘…@ğ¾)æ¥è¯„ä¼°ã€‚æœ¬æ–‡é‡‡ç”¨IVF-PQè¿™ç§ANNçš„æ–¹æ³•ï¼ŒåŸå› æ˜¯å®ƒæœ‰å‹ç¼©ã€‚

RALMæ¨ç†çš„é—®é¢˜ï¼š

1. LLMä¸RAGæ¨ç†ç‰¹æ€§ä¸åŒï¼ŒLLMä¸»è¦æ˜¯è®¡ç®—ã€å¸¦å®½ç­‰ï¼›RAGä¸»è¦æ˜¯æœç´¢ç®—æ³•ã€å­˜å‚¨
2. å¹¿æ³›çš„RARLé…ç½®ï¼Œæ¯”å¦‚æŸ¥è¯¢çš„é¢‘ç‡ï¼Œæ•°æ®åº“çš„è§„æ¨¡ï¼Œæ¨¡å‹çš„å¤§å°

motivationï¼š

* CPUs are slow in scanning PQ codes during query timeï¼Œå³ä½¿ç”¨äº†SIMDï¼›ä¸”å ç”¨å¤§é‡å†…å­˜å¸¦å®½1G/coreã€‚GPUè®¡ç®—ä¹Ÿä¸è¡Œï¼Œä¸»è¦æ˜¯æ˜¾å­˜å®¹é‡ä»¥åŠCPUä¸GPUé—´å¸¦å®½çš„é—®é¢˜ã€‚å¼‚æ„ç®—åŠ›åŠ é€Ÿè®¡ç®—ï¼Œå¦‚FPGAä½œä¸ºRAGçš„åŠ é€Ÿå™¨
* å¹¿æ³›çš„RARLé…ç½®å¯¼è‡´ä¸¤è€…æ‰€éœ€èµ„æºæ˜¯å˜åŒ–çš„ï¼Œéœ€è¦å¯ä»¥ç‹¬ç«‹åœ°å¼¹æ€§æ‰©å®¹è§£å†³ç“¶é¢ˆé—®é¢˜ã€‚

æ ¸å¿ƒæ€æƒ³ï¼šå¼‚æ„ç®—åŠ›åŠ é€Ÿã€åŒæ—¶PDåˆ†ç¦»çš„ç±»ä¼¼æ€æƒ³ï¼Œåˆ†ç¦»RAGä¸LLMæ¨ç†

ä¸»è¦ç»„æˆæ˜¯3éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ˜¯GPUä¸Šçš„æ¨ç†å¼•æ“ä»¥åŠIVF listï¼›ä¸€éƒ¨åˆ†æ˜¯CPUä¸Šçš„åè°ƒè¿›ç¨‹ï¼›ä¸€éƒ¨åˆ†æ˜¯FPGAçš„å‘é‡æŸ¥æ‰¾èŠ‚ç‚¹ï¼Œé€šè¿‡ç½‘çº¿è¿æ¥ï¼›

è¿™é‡Œæœ¬æ–‡åœ¨RAGæŸ¥è¯¢æ—¶GPUæ˜¯é—²ç½®çš„ï¼Œå¹¶æ²¡æœ‰ç”¨micro batchç­‰ï¼Œä½œè€…è¯´æ˜¯ä¼šé™ä½ååé‡ä¸”äºŒè€…æ—¶å»¶ä¸åŒ¹é…ã€‚åŒæ—¶æ¨¡å‹å¹¶æ²¡æœ‰ç”¨å¹¶è¡ŒæŠ€æœ¯ï¼Œä¸è¿‡è¿™ä¹Ÿä¸æ˜¯æœ¬æ–‡å…³å¿ƒçš„éƒ¨åˆ†ï¼Œä½†æ˜¯å¦‚æœå¹¶è¡Œçš„è¯è¿™éƒ¨åˆ†é—²ç½®çš„å¼€é”€æ˜¯ä¸æ˜¯å°±ä¸å¯å¿½ç•¥å‘¢ï¼Ÿ

> RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving

ä¹Ÿæ˜¯IVF-PQ

Empirical RAG Performance Trade-off Analysisï¼Œæ•°å­¦å»ºæ¨¡FLOPsinferenceï¼ŒBretrievalï¼ŒEnd-to-end RAG performance

RAGé—®é¢˜

* **ï¼ˆC1ï¼‰å†…åœ¨å¼‚æ„æ€§ (Heterogeneity)**ï¼šRAGç³»ç»Ÿç”±å¤šä¸ªä¸åŒç±»å‹çš„ç»„ä»¶æ„æˆï¼Œå¦‚ç”¨äºå‘é‡æœç´¢çš„æ£€ç´¢å™¨ã€ç”Ÿæˆå¼LLMï¼Œä»¥åŠå¯é€‰çš„æ–‡æ¡£ç¼–ç å™¨ã€æŸ¥è¯¢é‡å†™å™¨å’Œç»“æœé‡æ’åºå™¨ç­‰ ã€‚è¿™äº›ç»„ä»¶é€šå¸¸è¿è¡Œåœ¨ä¸åŒçš„ç¡¬ä»¶ä¸Šï¼ˆä¾‹å¦‚ï¼Œæ£€ç´¢å™¨åœ¨CPUä¸Šï¼ŒLLMåœ¨GPU/TPUç­‰åŠ é€Ÿå™¨ä¸Šï¼‰ï¼Œè¿™ä½¿å¾—ä¼˜åŒ–å˜å¾—æå…¶å¤æ‚ ã€‚
* **ï¼ˆC2ï¼‰æ€§èƒ½å¤šå˜æ€§ (Variability)**ï¼šä¸åŒçš„RAGé…ç½®ï¼ˆå¦‚çŸ¥è¯†åº“å¤§å°ã€æ£€ç´¢é¢‘ç‡ã€æ¨¡å‹é€‰æ‹©ç­‰ï¼‰ä¼šå¯¼è‡´æ€§èƒ½ç“¶é¢ˆåœ¨ä¸åŒç»„ä»¶ä¹‹é—´è½¬ç§» ã€‚ä¾‹å¦‚ï¼Œæœ‰æ—¶ç“¶é¢ˆåœ¨æ£€ç´¢ï¼Œæœ‰æ—¶åœ¨LLMæ¨ç† ã€‚

RAGSchemaï¼šä¸ºå¤æ‚çš„RAGå·¥ä½œè´Ÿè½½å»ºç«‹ç»Ÿä¸€è¯­è¨€ï¼Œåˆ©ç”¨RAGSchemaå®šä¹‰å¹¶åˆ†æäº†å››ç§ä»£è¡¨æ€§çš„RAGæ¨¡å¼ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„æ€§èƒ½ç“¶é¢ˆæ‰€åœ¨ï¼š

*   **åœºæ™¯ä¸€ï¼šè¶…å¤§è§„æ¨¡æ£€ç´¢**

    **ç‰¹ç‚¹**ï¼šä¸­å°å‹LLMæ­é…æå¤§çŸ¥è¯†åº“ï¼ˆæ•°ä¸‡äº¿ä»¤ç‰Œï¼‰ã€‚

    **å‘ç°**ï¼š**æ£€ç´¢æ˜¯ä¸»è¦ç“¶é¢ˆ**ï¼Œå°¤å…¶å½“LLMè¾ƒå°æˆ–å¤„ç†å¤šæŸ¥è¯¢æ—¶ï¼Œæ£€ç´¢å¯å æ€»æ—¶é—´80%ä»¥ä¸Šã€‚
*   **åœºæ™¯äºŒï¼šé•¿ä¸Šä¸‹æ–‡å¤„ç†**

    **ç‰¹ç‚¹**ï¼šå®æ—¶å¤„ç†é•¿æ–‡æ¡£ï¼ˆ>10ä¸‡ä»¤ç‰Œï¼‰ä½œä¸´æ—¶çŸ¥è¯†åº“ã€‚

    **å‘ç°**ï¼šæ£€ç´¢æ—¶é—´å‡ ä¹å¯å¿½ç•¥ï¼ˆ<1%ï¼‰ï¼Œç“¶é¢ˆè½¬ç§»è‡³

    **æ–‡æ¡£ç¼–ç **ã€‚å³ä½¿ç¼–ç å™¨ä»…1.2äº¿å‚æ•°ï¼Œå¤„ç†å¤§é‡æ–‡æœ¬çš„è€—æ—¶ä¹Ÿè¶…è¿‡700äº¿å‚æ•°ä¸»LLMã€‚
*   **åœºæ™¯ä¸‰ï¼šè¿­ä»£å¼æ£€ç´¢**

    **ç‰¹ç‚¹**ï¼šç”Ÿæˆè¿‡ç¨‹ä¸­å‘¨æœŸæ€§å¤šæ¬¡æ£€ç´¢ï¼Œè·å–ç²¾å‡†ä¸Šä¸‹æ–‡ã€‚

    **å‘ç°**ï¼šé¢‘ç¹æš‚åœè§£ç ç­‰å¾…æ£€ç´¢ç»“æœ**ä¸¥é‡å½±å“ç”Ÿæˆé€Ÿåº¦**ã€‚æ£€ç´¢æ‰¹æ¬¡å¤§å°è®¾ç½®å…³é”®ï¼šå¤ªå°å¢åŠ å»¶è¿Ÿï¼Œå¤ªå¤§å¯¼è‡´è§£ç å™¨ç©ºé—²ã€‚
*   **åœºæ™¯å››ï¼šå¢å¼ºå‹RAG**

    **ç‰¹ç‚¹**ï¼šæ£€ç´¢å‰ååˆ†åˆ«åŠ å…¥æŸ¥è¯¢é‡å†™å™¨å’Œé‡æ’åºå™¨ä¼˜åŒ–è´¨é‡ã€‚

    **å‘ç°**ï¼š**æŸ¥è¯¢é‡å†™å™¨æ˜¾è‘—å¢åŠ TTFT**ï¼Œå› å…¶æœ¬èº«æ˜¯è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ã€‚é‡æ’åºå™¨è®¡ç®—é‡å°ï¼Œå½±å“ä¸å¤§ã€‚

æ ¸å¿ƒæ€æƒ³ï¼šæ ¹æ®**RAGSchema**å’Œç»™å®šçš„**ç¡¬ä»¶èµ„æº**ï¼Œä¸ºå…¶é‡èº«å®šåˆ¶ä¸€å¥—æœ€ä¼˜çš„**è°ƒåº¦ç­–ç•¥**

ä¸ºäº†åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†RAGOæ¡†æ¶ï¼Œå®ƒèƒ½æ ¹æ®ç»™å®šçš„RAGSchemaå’Œç¡¬ä»¶èµ„æºï¼Œè‡ªåŠ¨æ¢ç´¢å¹¶æ‰¾å‡ºæœ€ä¼˜çš„ç³»ç»Ÿè°ƒåº¦ç­–ç•¥ ã€‚RAGOä¸»è¦ä¼˜åŒ–ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦çš„å†³ç­– ï¼š

1. **ä»»åŠ¡æ”¾ç½® (Task Placement)**ï¼šå†³å®šRAGæµæ°´çº¿ä¸­çš„ä¸åŒç»„ä»¶åº”è¯¥\*\*åˆå¹¶éƒ¨ç½²ï¼ˆcollocatedï¼‰**åœ¨åŒä¸€ç»„åŠ é€Ÿå™¨ä¸Šï¼Œè¿˜æ˜¯**åˆ†ç¦»éƒ¨ç½²ï¼ˆdisaggregatedï¼‰\*\*åœ¨ä¸åŒçš„ç¡¬ä»¶èµ„æºä¸Š ã€‚ä¾‹å¦‚ï¼Œæ˜¯å¦åº”è¯¥å°†è®¡ç®—ç‰¹æ€§ç›¸ä¼¼çš„æ–‡æ¡£ç¼–ç å™¨å’ŒLLMå‰ç¼€è®¡ç®—ï¼ˆprefixï¼‰é˜¶æ®µæ”¾åœ¨ä¸€èµ· ã€‚
2. **èµ„æºåˆ†é… (Resource Allocation)**ï¼šä¸ºæ¯ä¸ªï¼ˆåˆå¹¶æˆ–åˆ†ç¦»çš„ï¼‰ä»»åŠ¡åˆ†é…åˆé€‚æ•°é‡çš„ç¡¬ä»¶èµ„æºï¼ˆå¦‚XPUåŠ é€Ÿå™¨çš„æ•°é‡æˆ–CPUæœåŠ¡å™¨çš„æ•°é‡ï¼‰ ã€‚
3. **æ‰¹å¤„ç†ç­–ç•¥ (Batching Policy)**ï¼šä¸ºæµæ°´çº¿ä¸­çš„æ¯ä¸ªé˜¶æ®µè®¾ç½®æœ€ä½³çš„æ‰¹å¤„ç†å¤§å°ï¼Œä»¥å¹³è¡¡å»¶è¿Ÿå’Œååé‡ ã€‚

RAGOçš„å·¥ä½œæµç¨‹æ˜¯ï¼šé¦–å…ˆï¼Œåˆ©ç”¨ä¸€ä¸ªç»è¿‡æ ¡å‡†çš„æ€§èƒ½åˆ†ææ¨¡å‹ï¼Œå¯¹æ¯ä¸ªRAGç»„ä»¶åœ¨ä¸åŒèµ„æºå’Œæ‰¹æ¬¡å¤§å°ä¸‹çš„æ€§èƒ½è¿›è¡Œåˆ†æ ã€‚ç„¶åï¼Œå®ƒä¼šç©·ä¸¾æ‰€æœ‰å¯èƒ½çš„â€œä»»åŠ¡æ”¾ç½®ã€èµ„æºåˆ†é…ã€æ‰¹å¤„ç†â€ç»„åˆï¼Œè®¡ç®—æ¯ç§ç»„åˆçš„ç«¯åˆ°ç«¯æ€§èƒ½ï¼Œå¹¶æœ€ç»ˆç­›é€‰å‡ºæ€§èƒ½æœ€ä¼˜çš„ä¸€ç³»åˆ—é…ç½®



> Towards End-to-End Optimization of LLM-based Applications with Ayo. Asplos'25

* CUHK



> Towards Efficient Compound Large Language Model System Serving in the Wild. IWQoS'2024

* SJTU
* workshop
* optim e2e latency
* Challengeï¼šDAGçš„ä¸ç¡®å®šæ€§->topology;exec duration;
* motivationï¼šâ€œä¿¡æ¯çš„ç”Ÿæˆï¼ˆllm planerï¼‰â€æœ¬èº«å°±æ˜¯ä¸€ä¸ªå…³é”®çš„ã€éœ€è¦è¢«ä¼˜å…ˆä¿éšœçš„è®¡ç®—è¿‡ç¨‹ ã€‚ä¼˜å…ˆç”ŸæˆDAGæ–¹ä¾¿è°ƒåº¦åç»­æ­¥éª¤ï¼Œæé«˜èµ„æºåˆ©ç”¨ç‡ã€‚
* Solutionï¼š**PS-TCS** (Priority-based Scheduling with Topological Complexity Sensing)ï¼Œä¸åŒç±»å‹çš„APPçš„ä¸ç¡®å®šæ€§ä¸ä¸€æ ·ï¼Œä¼˜å…ˆè°ƒåº¦ä¸ç¡®å®šæ€§æœ€é«˜çš„

> Circinus: Efficient Query Planner for Compound ML Serving. ArXiv:2504

* UIUC
* not open-source
* keywords: slo aware edge-cloud schedule; agent serving; optim slo throughput;
* brainstorm
  * no edge? just device + cloud?
  * no pipeline concurrency detail

> Efficient Serving of LLM Applications with Probabilistic Demand Modeling. Arxiv:2506

* SJTU & HUAWEI CLOUD
* not open-source
* keywords: Probabilistic Demand Graph; LLM app serving; optimize e2e latencyï¼›
* é—®é¢˜ï¼š
  * é˜Ÿåˆ—è°ƒåº¦æ•ˆç‡ä½ä¸‹ï¼Œå› ä¸ºä¸çŸ¥é“ä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´åªèƒ½å½“åšé»‘ç›’
  * DAGåŠ¨æ€æ€§ï¼Œåç«¯å®¹å™¨é¢„çƒ­å»¶è¿Ÿé—®é¢˜
* insightï¼š
  * æ¨¡å—    è¾“å…¥è¾“å‡ºä»¥åŠå¹¶è¡Œåº¦çš„ç¨³å®šæ€§ï¼Œé«˜æ–¯åˆ†å¸ƒ
  * ä»…ä»…åˆ©ç”¨ç¦»çº¿æµ‹é‡çš„å‡å€¼ä¸å¤Ÿï¼Œè¾“å…¥è¾“å‡ºä¸ä¹‹å‰çš„ç»„ä»¶æœ‰å…³
* æ ¸å¿ƒæ€æƒ³ï¼šå»ºæ¨¡DAGå›¾ï¼Œé¢„æµ‹æ¯ä¸ªç»„ä»¶çš„èµ„æºéœ€æ±‚ï¼Œä»¥åŠå…¶åç»­ä¾èµ–çš„æ¦‚ç‡å…³ç³»ï¼Œä¼˜åŒ–è¯·æ±‚è°ƒåº¦é¡ºåºä»¥åŠå®¹å™¨é¢„çƒ­åˆ¤æ–­
* method
  * DAGå»ºæ¨¡ï¼šofflineè®°å½•å¤šä¸ªåŸå§‹å€¼ï¼Œä½¿ç”¨è’™ç‰¹å¡æ´›ç”Ÿæˆç»„ä»¶èµ„æºé¢„è®¡æ¶ˆè€—ï¼›ä¸‰ç§è·¨å•å…ƒéœ€æ±‚ç›¸å…³æ€§æ¨¡å¼ï¼Œçš®å°”æ£®ç›¸å…³åˆ†æï¼ŒOnlineæ—¶å½“å•å…ƒæ‰§è¡Œå®Œæˆåï¼ŒPDGraphè¿›è¡Œæ¡ä»¶é¢„æµ‹ä¼˜åŒ–ï¼Œè°ƒæ•´ä¸‹æ¸¸éœ€æ±‚é¢„æµ‹ä¼°è®¡ï¼›
  * PDGraph-based Queue Managementï¼š
* brainstormï¼š
  * ç«¯ä¾§å¤šæ¨¡å‹åˆ‡æ¢çš„é—®é¢˜
  * ç»„ä»¶åˆ©ç”¨é¢‘ç‡çš„é—®é¢˜ï¼›é¢„çƒ­æœ‰æ•ˆæ€§æ¦‚ç‡

> LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM Applications. arXiv:2504

* SJTU
* not open-source
* keywords: DAG scheduling; optimize e2e latencyï¼›
* background
  * schedule Challengeï¼šDAG uncertainty->topology; node exec duration;&#x20;
  * SFJ is useful to reduce ave latecy, but topology uncertainty makes wrong decision.
  * compound type
    * #### Predefined applications, fixed (rag)
    * #### chain like (react, iteration)
    * #### plan app (task-automation)
  * #### key insight: scheduling stages that <mark style="color:red;">reduce uncertainty</mark>, we can obtain valuable information once theyâ€™re completed.
  * #### design
    * #### DAG define
      * Regular Stage, llm stage, <mark style="color:red;">dynamic stage</mark>&#x20;
    * #### BN-based Profiler
      * #### offline, to identify stage reduces uncertainty
      * motivation: heatmaps show relation between stages, leverage inter-stage correlations
    * #### Entropy-based Uncertainty Quantification
      * #### to calculate which stage to schedule first
    * #### Uncertainty-aware Scheduler
      * **Îµ-greedy to mix schedule of SRFT and uncertainty schedule**
  * **brainstorm**
    * **only consider schdule layer, lacks fine-grained batch policy**

> Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents. ICML 2025 Workshop on MAS





> Towards Resource-Efficient Compound AI Systems. HOTOS 25

* MIT & Azure Research
* not open source
* keyword:&#x20;
  * focus on resource utilization
  * AI Workflows-as-a-Service (AIWaaS) design, similar to Faas. é€‰æ‹©æœ€ä¼˜æ¨¡å‹ã€é…ç½®å’Œè°ƒåº¦GPU/CPUèµ„æºã€æ§åˆ¶æˆæœ¬ã€ä¿è¯è´¨é‡ã€‚
  * **core: Workflow-Aware Cluster Management -> Cost/latency/quality/Energy...**

> KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows. Arxiv:2507

* UCSD & AWS
* not open source
* keyword: multi-agent serve, prefix kv cache, <mark style="color:red;">optim prefill latency</mark>
* <mark style="color:red;">key insight: cache the agent fixed prefix prompt shared by requests and consider workflow info to reduce cache miss to reduce prefilll latency</mark>
* backgroud
  * agent kv cache(_**tree-based**_)ï¼Œfixed part(large, agentâ€™s role, behavioral instructions, task description, and few-shot learning examples) +task-specific dynamic part(user inpput, small)ï¼Œ what we cache? _**KV of the fixed parts**_
  * different user lead to different fiexed part for the same agent, eg 2 executor instruct
  * LRU not fits agentic workflow, can not capture workflow info, leads to cache miss
* prefix cache management for agentic workflows
  * a workflow-aware eviction policy
    * DAGæ— æ³•æè¿°multi agentä¸­çš„åˆ†æ”¯å…³ç³»ï¼Œæ˜¯ANDè¿˜æ˜¯ORï¼Œæ— æ³•å‡†ç¡®è®¡ç®—è¯¥nodeè®¡ç®—æ˜¯åœ¨åé¢å‡ ä¸ªä½ç½®ï¼Œä»è€Œkv cacheçš„ä¼˜å…ˆçº§æ— æ³•ç¡®è®¤
    * æå‡ºAgent Step Graph abstractionï¼Œ**agent invocation as node level.** agents with larger steps-to-execution are more likely to be evicted.
  * overlapped KV prefetching mechanism
    * proactivate offload kvcache, like infinigen
* brainstrom
  * only optim llm prefill latency. useful for long sequence, but as the number of output tokens increases, the relative gain from KVFlow diminishes.
  * **not consider tool call time**, which may be the critical path

> RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation. arXiv:2404

* PKU & ByteDance
* not open-source
* keyword: rag prefix cache, serving, multi-level cache system, <mark style="color:red;">optim prefill latency</mark>
* <mark style="color:red;">key insight: cache the prefix kv-cache of the retrival</mark> <mark style="color:red;"></mark><mark style="color:red;">**hot**</mark> <mark style="color:red;"></mark><mark style="color:red;">doc to reduce prefill latency</mark>
* background
  * rag bottleneck lies in prefill step for long seq len rag
  * recurrence of identical _**hot**_ documents across multiple requests
* method
  * Greedy-Dual-Size-Frequency (PGDSF) **replacement policy** to minimize cache miss
    * tree based; cost aware;
  * cache-aware request reordering
    * priority queue; cached length / recompute length
  * dynamic speculative pipelining.
    * vector search may produce the final results early in the retrieval step
    * overlap the retrieval and generation steps
* brainstorm
  * not consider different rag config, not all situation bottleneck lies in prefill
  * 0.3M docs in vector database, top-1 or 2 selection, is a representive config?
  * may be consider agent workflow info? powerinfer? on device?

> HeterRAG: Heterogeneous Processing-in-Memory Acceleration for Retrieval-augmented Generation. ISCA â€™25

* HUST & NUS
* not open-source
* keywords: PIM based rag system; HBM-based PIM + DIMM-based PIM;



> Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC. arxiv:2506

* PKU
* not open-source
* keywords: heterogeneous SoCs; PD disaggregate; on-device; Intel Core Ultra with llama-3B;
* background
  * **ç«¯ä¾§ä¸¤ç§workloadï¼Œä¸€ç§è¢«åŠ¨ï¼Œä¸€ç§ä¸»åŠ¨ï¼›reactive-latencyï¼Œ proactive-throughput**
  * ç«¯ä¾§ä¸Šèµ„æºäº‰ç”¨ï¼›SOCç‰¹æ€§ï¼ˆå¦‚NPUï¼‰ä¸LLMä¸åŒ¹é…

### Idea

8å¡æœºä¹‹é—´nvlinkå…±äº«æ˜¾å­˜ï¼Œä¸åŒç»„ä»¶ä¹‹é—´å……åˆ†åˆ©ç”¨èƒ½åŠ›, aqua like schudlingï¼Œoptim placementï¼›Fair scheduleï¼›agentä¸Šä¸‹æ–‡è¿ç§»çœ‹çœ‹æœ‰ä»€ä¹ˆèƒ½åšçš„

multi-agent

GPU æ‹†åˆ†ï¼Ÿmultiplexing? nvidia-green split in on device senario

muti agent kv-cache + aqua

serveless + agent ?

could + edge workflow optim

dynamic resource allocation and load balancingåšè¿™ä¸ªï¼Ÿç°åœ¨å¥½åƒéƒ½æ˜¯é™æ€çš„

åˆ©ç”¨æ¦‚ç‡å»ºæ¨¡åŠ é€ŸAgent kv cacheå¤ç”¨

