## papers

---

>2025-01

>2025-02

>2025-03

- **Fast On-device LLM Inference with NPUs**. Daliang Xu et.al. **ASPLOS25**, **2024**, ([pdf](pdf/Fast_On-device_LLM_Inference_with_NPUs.pdf))([link](http://arxiv.org/abs/2407.05858v2)).
  - <mark>端侧推理，npu+cpu，量化int8</mark>
  - [x] [note](#2407.05858v2)
  - [x] [ppt](./doc/24-09-09-mllm.pptx)
- **LLM in a flash: Efficient Large Language Model Inference with Limited Memory**. Keivan Alizadeh et.al. **arxiv**, **2023**, ([pdf](pdf/LLM_in_a_flash__Efficient_Large_Language_Model_Inference_with_Limited___Memory.pdf))([link](http://arxiv.org/abs/2312.11514v3)).
  - <mark>端侧推理(pc)，存储优化，激活稀疏，参数offload</mark>
  - [x] [note](#2312.11514v3)
- **Mooncake: Trading More Storage for Less Computation — A KVCache-centric Architecture for Serving LLM Chatbot**. Ruoyu Qin et.al. **FAST**, **2025**, ([pdf](pdf/Mooncake.pdf))([link](https://www.usenix.org/system/files/fast25-qin.pdf))([slides](pdf/Mooncake_fast25_slides-qin.pdf/)).
  - <mark>serving，分布式kv-cache存储，prefill-decode分离</mark>
  - [x] [note](#fast25-qin)
- **APE: Faster and Longer Context-Augmented Generation via Adaptive
  Parallel Encoding**. Xinyu Yang et.al. **ICLR Poster**, **2025**, ([pdf](pdf/APE__Faster_and_Longer_Context-Augmented_Generation_via_Adaptive___Parallel_Encoding.pdf))([link](http://arxiv.org/abs/2502.05431v2))([open review](https://openreview.net/forum?id=yUC8pU508S)).
  - <mark>并行编码，training-free，kv-cache相似</mark>
  - [x] [note](#2502.05431v2)
  - [x] [ppt](doc/25-03-22-APE.pptx)
- **KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse**. Jingbo Yang et.al. **arxiv**, **2025**, ([pdf](pdf/KVLink__Accelerating_Large_Language_Models_via_Efficient_KV_Cache_Reuse.pdf))([link](http://arxiv.org/abs/2502.16002v1)).
  - [ ] [note](#2502.16002)
- **Speculative Decoding and Beyond: An In-Depth Survey of Techniques**. Yunhai Hu et.al. **arxiv**, **2025**, ([pdf](pdf/Speculative_Decoding_and_Beyond__An_In-Depth_Survey_of_Techniques.pdf))([link](http://arxiv.org/abs/2502.19732v3)).
- **LongSpec: Long-Context Speculative Decoding with Efficient Drafting and
  Verification**. Penghui Yang et.al. **arxiv**, **2025**, ([pdf](pdf/LongSpec__Long-Context_Speculative_Decoding_with_Efficient_Drafting_and___Verification.pdf))([link](http://arxiv.org/abs/2502.17421v1)).
- **Unlocking Efficiency in Large Language Model Inference: A Comprehensive
  Survey of Speculative Decoding**. Heming Xia et.al. **arxiv**, **2024**, ([pdf](pdf/Unlocking_Efficiency_in_Large_Language_Model_Inference__A_Comprehensive___Survey_of_Speculative_Decoding.pdf))([link](http://arxiv.org/abs/2401.07851v3)).
- **KVDirect: Distributed Disaggregated LLM Inference**. Shiyang Chen et.al. **arxiv**, **2024**, ([pdf](pdf/KVDirect__Distributed_Disaggregated_LLM_Inference.pdf))([link](http://arxiv.org/abs/2501.14743v1)).
- **vAttention: Dynamic Memory Management for Serving LLMs without
  PagedAttention**. Ramya Prabhu et.al. **arxiv**, **2024**, ([pdf](pdf/vAttention__Dynamic_Memory_Management_for_Serving_LLMs_without___PagedAttention.pdf))([link](http://arxiv.org/abs/2405.04437v3)).
- **IMPRESS: An Importance-Informed Multi-Tier Prefix KV Storage System for Large Language Model Inference**. Weijian Chen et.al. **FAST25**, ([pdf](pdf/fast25-chen-weijian-impress.pdf))([link](https://www.usenix.org/conference/fast25/presentation/chen-weijian-impress))([slide](pdf/fast25_slides-chen-weijian-impress.pdf)).
- **Long-Context Language Modeling with Parallel Context Encoding**. Howard Yen et.al. **ACL**, **2024**, ([pdf](pdf/Long-Context_Language_Modeling_with_Parallel_Context_Encoding.pdf))([link](http://arxiv.org/abs/2402.16617v2)).
  - <mark>并行编码，trainable</mark>
  - [x] [note](#2402.16617)
  - [x] [ppt](doc/25-03-22-APE.pptx)
- **Attention Entropy is a Key Factor: An Analysis of Parallel Context
  Encoding with Full-attention-based Pre-trained Language Models**. Zhisong Zhang et.al. **arxiv**, **2024**, ([pdf](pdf/Attention_Entropy_is_a_Key_Factor__An_Analysis_of_Parallel_Context___Encoding_with_Full-attention-based_Pre-trained_Language_Models.pdf))([link](http://arxiv.org/abs/2412.16545v1)).
- **LoongServe: Efficiently Serving Long-Context Large Language Models with
  Elastic Sequence Parallelism**. Bingyang Wu et.al. **sosp24**, **2024**, ([pdf](pdf/LoongServe__Efficiently_Serving_Long-Context_Large_Language_Models_with___Elastic_Sequence_Parallelism.pdf))([link](http://arxiv.org/abs/2404.09526v2)).
  - [x] [ppt](./doc/24-08-08-loongserve.pptx)
- **FastDecode: High-Throughput GPU-Efficient LLM Serving using
  Heterogeneous Pipelines**. Jiaao He et.al. **arxiv**, **2024**, ([pdf](pdf/FastDecode__High-Throughput_GPU-Efficient_LLM_Serving_using___Heterogeneous_Pipelines.pdf))([link](http://arxiv.org/abs/2403.11421v1)).
  - [x] [ppt](./doc/25-01-10-fast-decode.pptx)
- **Liger: Interleaving Intra- and Inter-Operator Parallelism for Distributed Large Model Inference**. Du Jiangsu et.al. **ppopp24**, **2024-2-20**, ([link](https://doi.org/10.1145/3627535.3638466))([pdf](pdf/Liger_ppopp24.pdf)).
  - [x] [ppt](./doc/24-11-02-Liger.pptx)

>2025-03

- **Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep
  Learning**. Wei An et.al. **SC24**, **2024**, ([pdf](pdf/Fire-Flyer_AI-HPC__A_Cost-Effective_Software-Hardware_Co-Design_for_Deep___Learning.pdf))([link](http://arxiv.org/abs/2408.14158v2)).
  - [] ppt
  - [] [note](#FF-AI-HPC)
