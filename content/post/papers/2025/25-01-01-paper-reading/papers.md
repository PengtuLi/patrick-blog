## papers

---

>2025-01

>2025-02

>2025-03

- **Fast On-device LLM Inference with NPUs**. Daliang Xu et.al. **ASPLOS25**, **2024**, ([pdf](pdf/Fast_On-device_LLM_Inference_with_NPUs.pdf))([link](http://arxiv.org/abs/2407.05858v2)).
  - [x] [note](#2407.05858v2)
- **LLM in a flash: Efficient Large Language Model Inference with Limited Memory**. Keivan Alizadeh et.al. **arxiv**, **2023**, ([pdf](pdf/LLM_in_a_flash__Efficient_Large_Language_Model_Inference_with_Limited___Memory.pdf))([link](http://arxiv.org/abs/2312.11514v3)).
  - [x] [note](#2312.11514v3)
- **Mooncake: Trading More Storage for Less Computation â€” A KVCache-centric Architecture for Serving LLM Chatbot**. Ruoyu Qin et.al. **FAST**, **2025**, ([pdf](pdf/Mooncake.pdf))([link](https://www.usenix.org/system/files/fast25-qin.pdf))([slides](pdf/Mooncake_fast25_slides-qin.pdf/)).
  - [ ] [note](#fast25-qin)
- **APE: Faster and Longer Context-Augmented Generation via Adaptive
  Parallel Encoding**. Xinyu Yang et.al. **arxiv**, **2025**, ([pdf](pdf/APE__Faster_and_Longer_Context-Augmented_Generation_via_Adaptive___Parallel_Encoding.pdf))([link](http://arxiv.org/abs/2502.05431v2)).