## papers

---

>2025-01

>2025-02

>2025-03

- **Fast On-device LLM Inference with NPUs**. Daliang Xu et.al. **ASPLOS25**, **2024**, ([link](http://arxiv.org/abs/2407.05858v2)).
  - <mark>端侧推理，npu+cpu，量化int8</mark>
  - [x] [note](#2407.05858v2)
  - [x] ppt
- **LLM in a flash: Efficient Large Language Model Inference with Limited Memory**. Keivan Alizadeh et.al. **arxiv**, **2023**, ([link](http://arxiv.org/abs/2312.11514v3)).
  - <mark>端侧推理(pc)，存储优化，激活稀疏，参数offload</mark>
  - [x] [note](#2312.11514v3)
- **Mooncake: Trading More Storage for Less Computation — A KVCache-centric Architecture for Serving LLM Chatbot**. Ruoyu Qin et.al. **FAST**, **2025**, ([link](https://www.usenix.org/system/files/fast25-qin.pdf)).
  - <mark>serving，分布式kv-cache存储，prefill-decode分离</mark>
  - [x] slide
  - [x] [note](#fast25-qin)
- **APE: Faster and Longer Context-Augmented Generation via Adaptive
  Parallel Encoding**. Xinyu Yang et.al. **ICLR Poster**, **2025**, ([link](http://arxiv.org/abs/2502.05431v2))([open review](https://openreview.net/forum?id=yUC8pU508S)).
  - <mark>并行编码，training-free，kv-cache相似</mark>
  - [x] [note](#2502.05431v2)
  - [x] ppt
- **KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse**. Jingbo Yang et.al. **arxiv**, **2025**, ([link](http://arxiv.org/abs/2502.16002v1)).
  - [ ] [note](#2502.16002)
- **Speculative Decoding and Beyond: An In-Depth Survey of Techniques**. Yunhai Hu et.al. **arxiv**, **2025**, ([link](http://arxiv.org/abs/2502.19732v3)).
- **LongSpec: Long-Context Speculative Decoding with Efficient Drafting and
  Verification**. Penghui Yang et.al. **arxiv**, **2025**, ([link](http://arxiv.org/abs/2502.17421v1)).
- **Unlocking Efficiency in Large Language Model Inference: A Comprehensive
  Survey of Speculative Decoding**. Heming Xia et.al. **arxiv**, **2024**, ([link](http://arxiv.org/abs/2401.07851v3)).
- **KVDirect: Distributed Disaggregated LLM Inference**. Shiyang Chen et.al. **arxiv**, **2024**, ([link](http://arxiv.org/abs/2501.14743v1)).
- **vAttention: Dynamic Memory Management for Serving LLMs without
  PagedAttention**. Ramya Prabhu et.al. **arxiv**, **2024**, ([link](http://arxiv.org/abs/2405.04437v3)).
- **IMPRESS: An Importance-Informed Multi-Tier Prefix KV Storage System for Large Language Model Inference**. Weijian Chen et.al. **FAST25**, ([link](https://www.usenix.org/conference/fast25/presentation/chen-weijian-impress)).
  - [x] slide
- **Long-Context Language Modeling with Parallel Context Encoding**. Howard Yen et.al. **ACL**, **2024**, ([link](http://arxiv.org/abs/2402.16617v2)).
  - <mark>并行编码，trainable</mark>
  - [x] [note](#2402.16617)
  - [x] ppt
- **Attention Entropy is a Key Factor: An Analysis of Parallel Context
  Encoding with Full-attention-based Pre-trained Language Models**. Zhisong Zhang et.al. **arxiv**, **2024**, ([link](http://arxiv.org/abs/2412.16545v1)).
- **LoongServe: Efficiently Serving Long-Context Large Language Models with
  Elastic Sequence Parallelism**. Bingyang Wu et.al. **sosp24**, **2024**, ([link](http://arxiv.org/abs/2404.09526v2)).
  - [x] ppt
- **FastDecode: High-Throughput GPU-Efficient LLM Serving using
  Heterogeneous Pipelines**. Jiaao He et.al. **arxiv**, **2024**, ([link](http://arxiv.org/abs/2403.11421v1)).
  - [x] ppt
- **Liger: Interleaving Intra- and Inter-Operator Parallelism for Distributed Large Model Inference**. Du Jiangsu et.al. **ppopp24**, **2024-2-20**, ([link](https://doi.org/10.1145/3627535.3638466)).
  - [x] ppt

>2025-04

- **Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep
  Learning**. Wei An et.al. **SC24**, **2024**, ([link](http://arxiv.org/abs/2408.14158v2)).
  - [] ppt
  - [] [note](#FF-AI-HPC)
