## papers

---

>2025-01

>2025-02

>2025-03

- **Fast On-device LLM Inference with NPUs**. Daliang Xu et.al. **ASPLOS25**, **2024**, ([pdf](pdf/Fast_On-device_LLM_Inference_with_NPUs.pdf))([link](http://arxiv.org/abs/2407.05858v2)).
  - [x] [note](#2407.05858v2)
- **LLM in a flash: Efficient Large Language Model Inference with Limited Memory**. Keivan Alizadeh et.al. **arxiv**, **2023**, ([pdf](pdf/LLM_in_a_flash__Efficient_Large_Language_Model_Inference_with_Limited___Memory.pdf))([link](http://arxiv.org/abs/2312.11514v3)).
  - [x] [note](#2312.11514v3)
- **Mooncake: Trading More Storage for Less Computation â€” A KVCache-centric Architecture for Serving LLM Chatbot**. Ruoyu Qin et.al. **FAST**, **2025**, ([pdf](pdf/Mooncake.pdf))([link](https://www.usenix.org/system/files/fast25-qin.pdf))([slides](pdf/Mooncake_fast25_slides-qin.pdf/)).
  - [x] [note](#fast25-qin)
- **APE: Faster and Longer Context-Augmented Generation via Adaptive
  Parallel Encoding**. Xinyu Yang et.al. **ICLR**, **2025**, ([pdf](pdf/APE__Faster_and_Longer_Context-Augmented_Generation_via_Adaptive___Parallel_Encoding.pdf))([link](http://arxiv.org/abs/2502.05431v2))([open review](https://openreview.net/forum?id=yUC8pU508S)).
  - [ ] [note](#2502.05431v2)
- **KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse**. Jingbo Yang et.al. **arxiv**, **2025**, ([pdf](pdf/KVLink__Accelerating_Large_Language_Models_via_Efficient_KV_Cache_Reuse.pdf))([link](http://arxiv.org/abs/2502.16002v1)).
  - [ ] [note](#2502.16002)
- **Speculative Decoding and Beyond: An In-Depth Survey of Techniques**. Yunhai Hu et.al. **arxiv**, **2025**, ([pdf](pdf/Speculative_Decoding_and_Beyond__An_In-Depth_Survey_of_Techniques.pdf))([link](http://arxiv.org/abs/2502.19732v3)).
- **LongSpec: Long-Context Speculative Decoding with Efficient Drafting and
  Verification**. Penghui Yang et.al. **arxiv**, **2025**, ([pdf](pdf/LongSpec__Long-Context_Speculative_Decoding_with_Efficient_Drafting_and___Verification.pdf))([link](http://arxiv.org/abs/2502.17421v1)).
- **Unlocking Efficiency in Large Language Model Inference: A Comprehensive
  Survey of Speculative Decoding**. Heming Xia et.al. **arxiv**, **2024**, ([pdf](pdf/Unlocking_Efficiency_in_Large_Language_Model_Inference__A_Comprehensive___Survey_of_Speculative_Decoding.pdf))([link](http://arxiv.org/abs/2401.07851v3)).
- **KVDirect: Distributed Disaggregated LLM Inference**. Shiyang Chen et.al. **arxiv**, **2024**, ([pdf](pdf/KVDirect__Distributed_Disaggregated_LLM_Inference.pdf))([link](http://arxiv.org/abs/2501.14743v1)).
- **vAttention: Dynamic Memory Management for Serving LLMs without
  PagedAttention**. Ramya Prabhu et.al. **arxiv**, **2024**, ([pdf](pdf/vAttention__Dynamic_Memory_Management_for_Serving_LLMs_without___PagedAttention.pdf))([link](http://arxiv.org/abs/2405.04437v3)).
- **IMPRESS: An Importance-Informed Multi-Tier Prefix KV Storage System for Large Language Model Inference**. Weijian Chen et.al. **FAST25**, ([pdf](pdf/fast25-chen-weijian-impress.pdf))([link](https://www.usenix.org/conference/fast25/presentation/chen-weijian-impress))([slide](pdf/fast25_slides-chen-weijian-impress.pdf)).

