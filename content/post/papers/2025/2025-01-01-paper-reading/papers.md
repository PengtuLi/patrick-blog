## papers

---

>2025-01

>2025-02

>2025-03

- **Fast On-device LLM Inference with NPUs**. Daliang Xu et.al. **ASPLOS25**, **2024**, ([pdf](pdf/Fast_On-device_LLM_Inference_with_NPUs.pdf))([link](http://arxiv.org/abs/2407.05858v2)).
  - [x] [note](#2407.05858v2)
- **LLM in a flash: Efficient Large Language Model Inference with Limited Memory**. Keivan Alizadeh et.al. **arxiv**, **2023**, ([pdf](pdf/LLM_in_a_flash__Efficient_Large_Language_Model_Inference_with_Limited___Memory.pdf))([link](http://arxiv.org/abs/2312.11514v3)).
  - [x] [note](#2312.11514v3)
